{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1a09d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building index...\n",
      "Processed 3948 documents.\n",
      "Indexed 30764 unique words.\n",
      "\n",
      "Enter a query (e.g., 'computer networks'):\n",
      "computer networks\n",
      "\n",
      "Relevant Documents: {1665, 1794, 259, 3330, 3079, 264, 777, 3208, 3463, 3472, 3090, 3606, 663, 3734, 1187, 2980, 2981, 3236, 1319, 3750, 3369, 683, 684, 1323, 1326, 2094, 3371, 2609, 3505, 2995, 2996, 2615, 2999, 3896, 698, 3002, 3657, 587, 1100, 1103, 208, 720, 3412, 469, 2901, 1367, 3158, 3416, 3418, 3671, 3420, 1373, 3296, 3424, 3936, 3431, 1128, 3690, 620, 3696, 3442, 1012, 2036, 1270, 2934, 3574, 3193, 3581, 1790}\n",
      "\n",
      "Ranked Documents: [2980, 2980, 2980, 2980, 2980, 2980, 2980, 2980]\n",
      "\n",
      "Top Document: {'ID': 2980, 'PageName': 'http_^^www.cs.wisc.edu^~shavlik^mlrg^publications.html', 'TotalWordCount': 813, 'TotalTagWeight': 2926}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import chardet\n",
    "\n",
    "# Constants for tag weights\n",
    "TAG_WEIGHTS = {\n",
    "    \"title\": 6,\n",
    "    \"h1\": 5, \"h2\": 5, \"h3\": 5,\n",
    "    \"a\": 4,\n",
    "    \"i\": 3, \"b\": 3,\n",
    "    \"body\": 1\n",
    "}\n",
    "\n",
    "# Stop words list\n",
    "STOP_WORDS = {\"is\", \"the\", \"of\", \"and\", \"in\", \"on\", \"at\", \"for\", \"a\", \"an\"}\n",
    "\n",
    "# Dataset directory (replace with your actual dataset path)\n",
    "DATASET_DIR = \"E:\\webkb\"\n",
    "\n",
    "# Initialize Index Tables\n",
    "page_info_table = []  # Stores metadata for documents\n",
    "word_info_table = defaultdict(list)  # Maps words to document IDs\n",
    "\n",
    "\n",
    "# STAGE 1: Dataset Preparation and Preprocessing\n",
    "def clean_and_tokenize(text):\n",
    "    \"\"\"Clean and tokenize text, removing stop words and special characters.\"\"\"\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    cleaned_text = text.translate(translator).lower()\n",
    "    tokens = [word for word in cleaned_text.split() if word not in STOP_WORDS]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def process_document(file_path, doc_id):\n",
    "    \"\"\"Process a single HTML document.\"\"\"\n",
    "    with open(file_path, 'r', encoding='latin1') as file:  # Try 'latin1' instead of 'utf-8'\n",
    "        content = file.read()\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "    # Initialize metadata for the document\n",
    "    page_metadata = {\n",
    "        \"ID\": doc_id,\n",
    "        \"PageName\": os.path.basename(file_path),\n",
    "        \"TotalWordCount\": 0,\n",
    "        \"TotalTagWeight\": 0\n",
    "    }\n",
    "\n",
    "    # Extract weighted content\n",
    "    for tag, weight in TAG_WEIGHTS.items():\n",
    "        for element in soup.find_all(tag):\n",
    "            if element.string:\n",
    "                tokens = clean_and_tokenize(element.string)\n",
    "                page_metadata[\"TotalWordCount\"] += len(tokens)\n",
    "                page_metadata[\"TotalTagWeight\"] += len(tokens) * weight\n",
    "                for token in tokens:\n",
    "                    word_info_table[token].append(doc_id)\n",
    "\n",
    "    page_info_table.append(page_metadata)\n",
    "\n",
    "def process_dataset(dataset_dir):\n",
    "    \"\"\"Process all documents in the dataset.\"\"\"\n",
    "    doc_id = 1\n",
    "    for root, _, files in os.walk(dataset_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".html\"):\n",
    "                process_document(os.path.join(root, file), doc_id)\n",
    "                doc_id += 1\n",
    "\n",
    "\n",
    "# STAGE 2: Advanced Document Indexing Method (ADIM)\n",
    "def build_index():\n",
    "    \"\"\"Build the index by processing the dataset.\"\"\"\n",
    "    process_dataset(DATASET_DIR)\n",
    "    print(f\"Processed {len(page_info_table)} documents.\")\n",
    "    print(f\"Indexed {len(word_info_table)} unique words.\")\n",
    "\n",
    "\n",
    "# STAGE 3: Query Searching Algorithm (QSA)\n",
    "def query_search(query):\n",
    "    \"\"\"Process a query and return relevant document IDs.\"\"\"\n",
    "    tokens = clean_and_tokenize(query)\n",
    "    relevant_docs = set(word_info_table[tokens[0]])\n",
    "    for token in tokens[1:]:\n",
    "        relevant_docs.intersection_update(word_info_table[token])\n",
    "    return relevant_docs\n",
    "\n",
    "\n",
    "# STAGE 4: Evolutionary Algorithm (MGA and CA Integration)\n",
    "def calculate_fitness(doc_id, query_tokens):\n",
    "    \"\"\"Calculate fitness of a document based on query relevance.\"\"\"\n",
    "    page = next(p for p in page_info_table if p[\"ID\"] == doc_id)\n",
    "    return page[\"TotalTagWeight\"]\n",
    "\n",
    "\n",
    "def genetic_algorithm(query_tokens, population_size=10, generations=15):\n",
    "    \"\"\"Run the genetic algorithm to rank relevant documents.\"\"\"\n",
    "    # Initial Population\n",
    "    population = list(query_search(\" \".join(query_tokens)))\n",
    "    if len(population) > population_size:\n",
    "        population = random.sample(population, population_size)\n",
    "\n",
    "    for _ in range(generations):\n",
    "        # Evaluate Fitness\n",
    "        fitness_scores = {doc: calculate_fitness(doc, query_tokens) for doc in population}\n",
    "\n",
    "        # Selection (Elitism)\n",
    "        sorted_population = sorted(population, key=lambda doc: fitness_scores[doc], reverse=True)\n",
    "        parents = sorted_population[:len(sorted_population) // 2]\n",
    "\n",
    "        # Crossover\n",
    "        offspring = []\n",
    "        for i in range(0, len(parents), 2):\n",
    "            if i + 1 < len(parents):\n",
    "                offspring.append(parents[i])\n",
    "                offspring.append(parents[i + 1])\n",
    "\n",
    "        # Mutation\n",
    "        for i in range(len(offspring)):\n",
    "            if random.random() < 0.1:  # Mutation Probability\n",
    "                offspring[i] = random.choice(population)\n",
    "\n",
    "        # Next Generation\n",
    "        population = parents + offspring\n",
    "\n",
    "    # Return sorted final population\n",
    "    return sorted(population, key=lambda doc: calculate_fitness(doc, query_tokens), reverse=True)\n",
    "\n",
    "\n",
    "# Main Script\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Building index...\")\n",
    "    build_index()\n",
    "\n",
    "    # Example Query\n",
    "    print(\"\\nEnter a query (e.g., 'computer networks'):\")\n",
    "    user_query = input().strip()\n",
    "    query_tokens = clean_and_tokenize(user_query)\n",
    "\n",
    "    # Query Search\n",
    "    relevant_docs = query_search(user_query)\n",
    "    print(f\"\\nRelevant Documents: {relevant_docs}\")\n",
    "\n",
    "    # Genetic Algorithm Ranking\n",
    "    ranked_docs = genetic_algorithm(query_tokens)\n",
    "    print(f\"\\nRanked Documents: {ranked_docs}\")\n",
    "\n",
    "    # Display top-ranked document\n",
    "    if ranked_docs:\n",
    "        top_doc_id = ranked_docs[0]\n",
    "        top_doc = next(p for p in page_info_table if p[\"ID\"] == top_doc_id)\n",
    "        print(f\"\\nTop Document: {top_doc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a150d258",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
